{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21218ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import re, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message = \".*matmul.*\")\n",
    "np.seterr(all = \"ignore\")\n",
    "\n",
    "CSV_PATH = \"/Users/merterol/Desktop/UZH/CompLing:CompSci/CL/Sem 5/Bachelors Thesis/VSCode/Bachelors-Thesis/data/merged.csv\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faca8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sender_domain(sender: str) -> str:\n",
    "    if not isinstance(sender, str) or sender.strip() == \"\":\n",
    "        return \"no_sender\"\n",
    "    m = re.search(r'@([a-zA-Z0-9.-]+)', sender)\n",
    "    \n",
    "    return m.group(1).lower() if m else \"unknown_format\"\n",
    "\n",
    "# --- Loader ---\n",
    "df = pd.read_csv(CSV_PATH).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "df[\"sender\"] = df[\"sender\"].fillna(\"\")\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "df[\"sender_domain\"] = df[\"sender\"].apply(extract_sender_domain)\n",
    "\n",
    "df[\"joined_text\"] = \"sender=\" + df[\"sender_domain\"] + \" \" + df[\"text\"]\n",
    "\n",
    "X = df[\"joined_text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y if np.unique(y).size > 1 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07c7609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report (Text Only for LIME) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98     12150\n",
      "           1       0.98      0.98      0.98     13530\n",
      "\n",
      "    accuracy                           0.98     25680\n",
      "   macro avg       0.98      0.98      0.98     25680\n",
      "weighted avg       0.98      0.98      0.98     25680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Pipeline ---\n",
    "if len(np.unique(y_train)) < 2:\n",
    "    print(\"Error: Training data contains only one class. Please check your data or label creation process.\")\n",
    "    print(\"Unique classes in y_train:\", np.unique(y_train))\n",
    "else:\n",
    "    text_clf = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=3000, min_df = 2, stop_words=\"english\", lowercase=True, token_pattern=r'(?u)\\b[\\w@.\\-=:]+\\b')),\n",
    "        (\"lr\", LogisticRegression(solver=\"saga\", max_iter=1000, n_jobs=1, random_state=RANDOM_STATE)),\n",
    "    ])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    print(\"\\n=== Classification Report (Text Only for LIME) ===\")\n",
    "    print(classification_report(y_test, text_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c8ab9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_safe(texts):\n",
    "    P = text_clf.predict_proba(list(texts))\n",
    "    return np.nan_to_num(P, nan=0.5, posinf=1.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44cf46c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORIGINAL EMAIL (joined with sender) ===\n",
      "\n",
      "sender=no_sender http myprint hk don't miss this unique escapelong pills x escapenumbermg escapenumber escapenumberonly escapenumber escapenumber per pil lescapenumber pills x escapenumbermg escapenumber escapenumberonly escapenumber escapenumber per pillescapenumber pills x escapenumbermg escapenumber escapenumberonly escapenumber escapenumber per pillescapenumber pills x escapenumbermg escapenumber escapenumberonly escapenumber escapenumber per pillescapenumber pills x escapenumbermg escapenumber escapenumberonly escapenumber escapenumber per pillescapenumber pills x escapenumbermg escapenumber escapenumberonly escapenumber escapenumber per pillescapenumber pills x escapenumbermg escapenumber escapenumberonly escapenumber escapenumber per pill cialisescapenumber pills x escapenumbermg escapenumber escapenumberonly escapenumber per pillescapenumber pills x escapenumbermg escapenumber escapenumberonly escapenumber escapenumber per pillescapenumber pills x escapenumbermg escapenumber es\n",
      "\n",
      "LIME explanation (top tokens)\n",
      "[(np.str_('pills'), 0.0803776603526024), (np.str_('escapenumbermg'), 0.07653641327559962), (np.str_('escapenumber'), -0.03985827406204308), (np.str_('x'), -0.011443010150840973), (np.str_('hk'), 0.010533152912571463), (np.str_('escapenumberonly'), 0.010454880559884697), (np.str_('pil'), -0.0076344949797251885), (np.str_('pillescapenumber'), 0.007532236045096248), (np.str_('pill'), 0.006904049566708446), (np.str_('escapelong'), 0.006641234584592762), (np.str_('discount'), 0.004533010538531274), (np.str_('unique'), 0.0045093868015313775), (np.str_('per'), -0.004058801919154405), (np.str_('p'), 0.003498606910355145), (np.str_('don'), 0.003121354323448163)]\n",
      "\n",
      "Predicted label: Spam | Probabilities: {np.int64(0): np.float64(0.009110091120758224), np.int64(1): np.float64(0.9908899088792418)}\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mpills\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[31mescapenumbermg\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[32mescapenumber\u001b[0m : \u001b[32mContributed to a lower spam prediction factor\u001b[0m\n",
      "\u001b[32mx\u001b[0m : \u001b[32mContributed to a lower spam prediction factor\u001b[0m\n",
      "\u001b[31mhk\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[31mescapenumberonly\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[32mpil\u001b[0m : \u001b[32mContributed to a lower spam prediction factor\u001b[0m\n",
      "\u001b[31mpillescapenumber\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[31mpill\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[31mescapelong\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[31mdiscount\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[31munique\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[32mper\u001b[0m : \u001b[32mContributed to a lower spam prediction factor\u001b[0m\n",
      "\u001b[31mp\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n",
      "\u001b[31mdon\u001b[0m : \u001b[31mContributed to a higher spam prediciton factor\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --- LIME ---\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# LIME likes class names as strings\n",
    "class_names = [str(c) for c in np.unique(y_train)]\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Map the positive class index robustly (supports 'spam'/1)\n",
    "classes = text_clf.classes_\n",
    "if any(isinstance(c, str) and c.lower() == \"spam\" for c in classes):\n",
    "    spam_idx = int(np.where(np.char.lower(classes.astype(str)) == \"spam\")[0][0])\n",
    "elif 1 in classes:\n",
    "    spam_idx = int(np.where(classes == 1)[0][0])\n",
    "else:\n",
    "    spam_idx = 1 if len(classes) > 1 else 0\n",
    "\n",
    "# Pick any test row by POSITION\n",
    "i = 7 \n",
    "instance_text = X_test[i]\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    instance_text,\n",
    "    predict_proba_safe,\n",
    "    num_features=15,\n",
    "    num_samples=2000,\n",
    "    labels=[spam_idx]\n",
    ")\n",
    "\n",
    "import sys\n",
    "from termcolor import colored\n",
    "\n",
    "def weight_to_text(word, weight):\n",
    "    if weight > 0 and weight <= 1:\n",
    "        w = colored(word, \"red\")\n",
    "        s = colored(\"Contributed to a higher spam prediciton factor\", \"red\")\n",
    "    else:\n",
    "        w = colored(word, \"green\")\n",
    "        s = colored(\"Contributed to a lower spam prediction factor\", \"green\")\n",
    "    \n",
    "    return w, s\n",
    "\n",
    "print(\"\\n=== ORIGINAL EMAIL (joined with sender) ===\\n\")\n",
    "print(instance_text[:1000])\n",
    "\n",
    "print(\"\\nLIME explanation (top tokens)\")\n",
    "print(exp.as_list(label=spam_idx))\n",
    "\n",
    "pred_label = text_clf.predict([instance_text])[0]\n",
    "pred_probs = text_clf.predict_proba([instance_text])[0]\n",
    "print(f\"\\nPredicted label: {\"Spam\" if pred_label == 1 else \"Ham\"} | Probabilities: {dict(zip(classes, pred_probs))}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "for word, weight in exp.as_list(label=spam_idx):\n",
    "    w, s = weight_to_text(word, weight)\n",
    "    print(w, \":\", s)\n",
    "    #print(weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
